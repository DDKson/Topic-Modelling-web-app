import numpy as np
import pandas as pd
import re
import string
from underthesea import word_tokenize, text_normalize


VIET_TAT = {
    " sp " : "sáº£n pháº©m",
    " dc ": "Ä‘Æ°á»£c",
    " k ": "khÃ´ng",
    " ko ": "khÃ´ng",
    " r ": "rá»“i",
    " oke ": "ok",
    " okie ": "ok",
    " okey ": "ok",
    " nt ": "nháº¯n tin",
    " bt ": "biáº¿t",
    " bÃ­t ": "biáº¿t",
    " ae ": "anh em",
    " nx ": "ná»¯a",
    " nv ": "nhÃ¢n viÃªn",
    " tv ": "tÆ° váº¥n",
    " mn ": "má»i ngÆ°á»i"
}

replace_list = {
        'Ã²a': 'oÃ ', 'Ã³a': 'oÃ¡', 'á»a': 'oáº£', 'Ãµa': 'oÃ£', 'á»a': 'oáº¡', 'Ã²e': 'oÃ¨', 'Ã³e': 'oÃ©','á»e': 'oáº»',
        'Ãµe': 'oáº½', 'á»e': 'oáº¹', 'Ã¹y': 'uá»³', 'Ãºy': 'uÃ½', 'á»§y': 'uá»·', 'Å©y': 'uá»¹','á»¥y': 'uá»µ', 'uáº£': 'á»§a',
        'aÌ‰': 'áº£', 'Ã´Ì': 'á»‘', 'uÂ´': 'á»‘','Ã´Ìƒ': 'á»—', 'Ã´Ì€': 'á»“', 'Ã´Ì‰': 'á»•', 'Ã¢Ì': 'áº¥', 'Ã¢Ìƒ': 'áº«', 'Ã¢Ì‰': 'áº©',
        'Ã¢Ì€': 'áº§', 'oÌ‰': 'á»', 'ÃªÌ€': 'á»','ÃªÌƒ': 'á»…', 'ÄƒÌ': 'áº¯', 'uÌ‰': 'á»§', 'ÃªÌ': 'áº¿', 'Æ¡Ì‰': 'á»Ÿ', 'iÌ‰': 'á»‰',
        'eÌ‰': 'áº»', 'Ã k': u' Ã  ','aË‹': 'Ã ', 'iË‹': 'Ã¬', 'ÄƒÂ´': 'áº¯','Æ°Ì‰': 'á»­', 'eËœ': 'áº½', 'yËœ': 'á»¹', 'aÂ´': 'Ã¡',
        #Quy cÃ¡c icon vá» 2 loáº¡i emoj: TÃ­ch cá»±c hoáº·c tiÃªu cá»±c
        "ğŸ‘¹": " negative ", "ğŸ‘»": " positive ", "ğŸ’ƒ": " positive ",'ğŸ¤™': ' positive ', 'ğŸ‘': ' positive ',
        "ğŸ’„": " positive ", "ğŸ’": " positive ", "ğŸ’©": " positive ","ğŸ˜•": " negative ", "ğŸ˜±": " negative ", "ğŸ˜¸": " positive ",
        "ğŸ˜¾": " negative ", "ğŸš«": " negative ",  "ğŸ¤¬": " negative ","ğŸ§š": " positive ", "ğŸ§¡": " positive ",'ğŸ¶':' positive ',
        'ğŸ‘': ' negative ', 'ğŸ˜£': ' negative ','âœ¨': ' positive ', 'â£': ' positive ','â˜€': ' positive ',
        'â™¥': ' positive ', 'ğŸ¤©': ' positive ', 'like': ' positive ', 'ğŸ’Œ': ' positive ',
        'ğŸ¤£': ' positive ', 'ğŸ–¤': ' positive ', 'ğŸ¤¤': ' positive ', ':(': ' negative ', 'ğŸ˜¢': ' negative ',
        'â¤': ' positive ', 'ğŸ˜': ' positive ', 'ğŸ˜˜': ' positive ', 'ğŸ˜ª': ' negative ', 'ğŸ˜Š': ' positive ',
        '?': ' ? ', 'ğŸ˜': ' positive ', 'ğŸ’–': ' positive ', 'ğŸ˜Ÿ': ' negative ', 'ğŸ˜­': ' negative ',
        'ğŸ’¯': ' positive ', 'ğŸ’—': ' positive ', 'â™¡': ' positive ', 'ğŸ’œ': ' positive ', 'ğŸ¤—': ' positive ',
        '^^': ' positive ', 'ğŸ˜¨': ' negative ', 'â˜º': ' positive ', 'ğŸ’‹': ' positive ', 'ğŸ‘Œ': ' positive ',
        'ğŸ˜–': ' negative ', 'ğŸ˜€': ' positive ', ':((': ' negative ', 'ğŸ˜¡': ' negative ', 'ğŸ˜ ': ' negative ',
        'ğŸ˜’': ' negative ', 'ğŸ™‚': ' positive ', 'ğŸ˜': ' negative ', 'ğŸ˜': ' positive ', 'ğŸ˜„': ' positive ',
        'ğŸ˜™': ' positive ', 'ğŸ˜¤': ' negative ', 'ğŸ˜': ' positive ', 'ğŸ˜†': ' positive ', 'ğŸ’š': ' positive ',
        'âœŒ': ' positive ', 'ğŸ’•': ' positive ', 'ğŸ˜': ' negative ', 'ğŸ˜“': ' negative ', 'ï¸ğŸ†—ï¸': ' positive ',
        'ğŸ˜‰': ' positive ', 'ğŸ˜‚': ' positive ', ':v': '  positive ', '=))': '  positive ', 'ğŸ˜‹': ' positive ', "ğŸ™†": ' positive ', "ğŸ¤": ' positive ', "ğŸ¥°": ' positive ',
        'ğŸ’“': ' positive ', 'ğŸ˜': ' negative ', ':3': ' positive ', 'ğŸ˜«': ' negative ', 'ğŸ˜¥': ' negative ', 'ğŸ˜…': ' positive ',
        'ğŸ˜ƒ': ' positive ', 'ğŸ˜¬': ' negative ', 'ğŸ˜Œ': ' positive ', 'ğŸ’›': ' positive ', 'ğŸ¤': ' positive ', 'ğŸˆ': ' positive ',
        'ğŸ˜—': ' positive ', 'ğŸ¤”': ' negative ', 'ğŸ˜‘': ' negative ', 'ğŸ”¥': ' negative ', 'ğŸ™': ' negative ',
        'ğŸ†—': ' positive ', 'ğŸ˜»': ' positive ', 'ğŸ’™': ' positive ', 'ğŸ’Ÿ': ' positive ',
        'ğŸ˜š': ' positive ', 'âŒ': ' negative ', 'ğŸ‘': ' positive ', ';)': ' positive ', '<3': ' positive ',
        'ğŸŒ': ' positive ',  'ğŸŒ·': ' positive ', 'ğŸŒ¸': ' positive ', 'ğŸŒº': ' positive ',
        'ğŸŒ¼': ' positive ', 'ğŸ“': ' positive ', 'ğŸ…': ' positive ', 'ğŸ¾': ' positive ', 'ğŸ‘‰': ' positive ',
        'ğŸ’': ' positive ', 'ğŸ’': ' positive ', 'ğŸ’¥': ' positive ', 'ğŸ’ª': ' positive ',
        'ğŸ’°': ' positive ',  'ğŸ˜‡': ' positive ', 'ğŸ˜›': ' positive ', 'ğŸ˜œ': ' positive ',
        'ğŸ™ƒ': ' positive ', 'ğŸ¤‘': ' positive ', 'ğŸ¤ª': ' positive ','â˜¹': ' negative ',  'ğŸ’€': ' negative ',
        'ğŸ˜”': ' negative ', 'ğŸ˜§': ' negative ', 'ğŸ˜©': ' negative ', 'ğŸ˜°': ' negative ', 'ğŸ˜³': ' negative ', "ğŸ¥²": ' negative ', 'ğŸ™„': ' negative ', 'ğŸ¤¦': ' negative ',
        'ğŸ˜µ': ' negative ', 'ğŸ˜¶': ' negative ', 'ğŸ™': ' negative ', "â¸": " product ", "â˜…": ' star ', 'â™€': " ", 'ğŸ˜…': " positive ", " ğŸ‘‹ ": " ", " ğŸ» ": " product ", "ğŸ¿": " product ", "ğŸ˜½": " positve ",
        "ğŸ˜¬" : ' negative ', 'ğŸ¤·': ' negative ', 'ğŸ˜Œ': ' poisitive ', "ğŸ˜¿": ' negative ', 'âœ“': " positive ", 'â˜†': ' star ', 'à² ï½£à² à¼àº¶â€¿à¼àº¶': ' ', 'ğŸµ': ' negative ',
        "ğŸ‘‚" : ' ', "ğŸ˜´": ' negative ', 'ğŸ‘‹': ' ', '\xa0': ' ', 'ğŸ»': ' product ', 'ğŸ»': ' product ',
        #Chuáº©n hÃ³a 1 sá»‘ sentiment words/English words
        ':))': '  positive ', ':)': ' positive ', 'Ã´ kÃªi': ' ok ', 'okie': ' ok ', ' o kÃª ': ' ok ', "cmn": "cm",
        ' okey ': ' ok ', ' Ã´kÃª ': ' ok ', ' oki ': ' ok ', ' oke ':  ' ok ',' okay ':' ok ',' okÃª ':' ok ',
        ' tks ': u' cÃ¡m Æ¡n ', ' thks ': u' cÃ¡m Æ¡n ', ' thanks ': u' cÃ¡m Æ¡n ', ' ths ': u' cÃ¡m Æ¡n ', ' thank ': u' cÃ¡m Æ¡n ',
        'â­': 'star ', '*': 'star ', 'ğŸŒŸ': 'star ', 'ğŸ‰': u' positive ', "bik": " bá»‹ ",
        ' kg ': u' khÃ´ng ','not': u' khÃ´ng ', u' kg ': u' khÃ´ng ', ' k ': u' khÃ´ng ',' kh ':u' khÃ´ng ',' kÃ´ ':u' khÃ´ng ','hok':u' khÃ´ng ',' kp ': u' khÃ´ng pháº£i ',u' kÃ´ ': u' khÃ´ng ', '"ko ': u' khÃ´ng ', u' ko ': u' khÃ´ng ', u' k ': u' khÃ´ng ', 'khong': u' khÃ´ng ', u' hok ': u' khÃ´ng ',
        'he he': ' positive ','hehe': ' positive ','hihi': ' positive ', ' haha': ' positive ', 'hjhj': ' positive ',
        ' lol ': ' negative ',' cc ': ' negative ','cute': u' dá»… thÆ°Æ¡ng ',' huhu ': ' negative ', ' vs ': u' vá»›i ', ' wa ': ' quÃ¡ ', 'wÃ¡': u' quÃ¡', ' j ': u' gÃ¬ ', 'â€œ': ' ',
        ' sz ': u' cá»¡ ', 'size': u' cá»¡ ', u' Ä‘x ': u' Ä‘Æ°á»£c ', 'dk': u' Ä‘Æ°á»£c ', ' dc ': u' Ä‘Æ°á»£c ', 'Ä‘k': u' Ä‘Æ°á»£c ',
        ' Ä‘c ': u' Ä‘Æ°á»£c ',' authentic ': u' chÃ­nh hÃ£ng ',u' aut ': u' chÃ­nh hÃ£ng ', u' auth ': u' chÃ­nh hÃ£ng ', ' thick ': u' positive ', ' store ': u' cá»­a hÃ ng ',
        ' shop ': u' cá»­a hÃ ng ', ' sp ': u' sáº£n pháº©m ', ' gud ': u' tá»‘t ',' god ': u' tá»‘t ',' wel done ':' tá»‘t ', ' good ': u' tá»‘t ', ' gÃºt ': u' tá»‘t ',
        ' sáº¥u ': u' xáº¥u ',' gut ': u' tá»‘t ', u' tot ': u' tá»‘t ', u' nice ': u' tá»‘t ', ' perfect ': 'ráº¥t tá»‘t',
        'time': u' thá»i gian ', 'qÃ¡': u' quÃ¡ ', u' ship ': u' giao hÃ ng ', u' m ': u' mÃ¬nh ', u' mik ': u' mÃ¬nh ',
        ' ÃªÌ‰ ': 'á»ƒ', ' product ': 'sáº£n pháº©m', ' quality ': 'cháº¥t lÆ°á»£ng', ' excelent ': 'hoÃ n háº£o', ' bad ': ' tá»‡ ',' fresh ': ' tÆ°Æ¡i ','sad': ' tá»‡ ',
        ' date ': u' háº¡n sá»­ dá»¥ng ', ' hsd ': u' háº¡n sá»­ dá»¥ng ',' quickly ': u' nhanh ', ' quick ': u' nhanh ',' fast ': u' nhanh ',' delivery ': u' giao hÃ ng ',u' sÃ­p ': u' giao hÃ ng ',
        ' beautiful ': u' Ä‘áº¹p tuyá»‡t vá»i ', u' tl ': u' tráº£ lá»i ', u' r ': u' rá»“i ', u' shope ': u' cá»­a hÃ ng ',u' order ': u' Ä‘áº·t hÃ ng ',
        ' cháº¥t lg ': u' cháº¥t lÆ°á»£ng ',u' sd ': u' sá»­ dá»¥ng ',u' dt ': u' Ä‘iá»‡n thoáº¡i ',u' nt ': u' nháº¯n tin ',u' tl ': u' tráº£ lá»i ',u' sÃ i ': u' xÃ i ',u' bjo ':u' bao giá» ',
        ' thik ': u' thÃ­ch ',u' sop ': u' cá»­a hÃ ng ', ' fb ': ' facebook ', ' face ': ' facebook ', ' very ': u' ráº¥t ',u'quáº£ ng ':u' quáº£ng  ',
        ' dep ': u' Ä‘áº¹p ',u' xau ': u' xáº¥u ',' delicious ': u' ngon ', u' hÃ g ': u' hÃ ng ', u' qá»§a ': u' quáº£ ',
        ' iu ': u' yÃªu ','fake': u' giáº£ máº¡o ', ' trl ': 'tráº£ lá»i', '><': u' positive ',
        ' por ': u' tá»‡ ',' poor ': u' tá»‡ ', ' ib ':u' nháº¯n tin ', ' rep ':u' tráº£ lá»i ',u' fback ':' feedback ',' fedback ':' feedback ',
        #dÆ°á»›i 3* quy vá» 1*, trÃªn 3* quy vá» 5*
        '6 sao': ' 5star ','6 star': ' 5star ', '5star': ' 5star ','5 sao': ' 5star ','5sao': ' 5star ',
        'starstarstarstarstar': ' 5star ', '1 sao': ' 1star ', '1sao': ' 1star ','2 sao':' 1star ','2sao':' 1star ',
        '2 starstar':' 1star ','1star': ' 1star ', '0 sao': ' 1star ', '0star': ' 1star ',}

def remove_duplicate_ending_letters(word):

    """A function to remove duplicated ending letters in a word (For example: from "okkk" to "ok")
    Parameters
    ----------
    word : str
    A word with duplicated ending letters 

    Returns
    ----------
    str
    A word with no duplicated ending letter
    """
    pattern = r"\b(\w+)(.)\2{1}\b"
    replaced_word = re.sub(pattern, r"\1\2", word)
    while replaced_word != word:
        word = replaced_word
        pattern = r"\b(\w+)(.)\2{1}\b"
        replaced_word = re.sub(pattern, r"\1\2", replaced_word)
    return replaced_word

def expand_contractions(sentence, contraction_mapping):

    """Expand words written in short hands or teen code (For example: "CÃ¢u nÃ y dc dÃ¹ng lÃ m vÃ­ dá»¥" to "CÃ¢u nÃ y Ä‘Æ°á»£c dÃ¹ng lÃ m vÃ­ dá»¥") 
    Parameters
    ----------
    sentence : str
    A sentence with contracted words

    contraction_mapping: dict
    Contains the words needs replacement as key and the replacement as values

    Returns
    ----------
    str
    Sentence with no contractions
    """

    sentence = " " + sentence + " "
    contraction_pattern = re.compile("({})".format("|".join(contraction_mapping.keys())), flags= re.IGNORECASE|re.DOTALL)
    def expand_match(contraction):
        match = contraction.group(0)
        first_char = match[1]
        expanded_contraction = contraction_mapping.get(match) if contraction_mapping.get(match) else contraction_mapping.get(match.lower())
        expanded_contraction = " " + first_char + expanded_contraction[1:] + " "
        return expanded_contraction
    expanded_sentence = contraction_pattern.sub(expand_match, sentence)
    while sentence != expanded_sentence:
        sentence = expanded_sentence
        expanded_sentence = contraction_pattern.sub(expand_match, sentence)
    return expanded_sentence.strip()

def remove_all_numbers(sentence):

    """Remove all numbers in a sentence
    Parameters
    ----------
    sentence : str
    A sentence with numbers

    Returns
    ----------
    str
    Sentence with no number
    """
    
    return re.sub(r'[0-9]', '', sentence)

def replace_sent(text, replace_list):

    """Remove elements in a text according to the replacement dictionary
    Parameters
    ----------
    text : str
    Text that needs replacement

    Returns
    ----------
    str
    Text with all elements in the replacement dictionary replaced
    """

    for k, v in replace_list.items():
        text = text.replace(k, v)
    return text

def remove_punctuation(text):
    """Remove all punctuations in a text
    Parameters
    ----------
    text : str
    Text that has punctuations 

    Returns
    ----------
    str
    Text with no punctuation
    """
    return text.translate(str.maketrans(string.punctuation, ' '*len(string.punctuation)))

def remove_all_tag(sentence):
    """Remove tags added from reading files
    Parameters
    ----------
    text : str
    Text that has tags

    Returns
    ----------
    str
    Text with no tags
    """
    return re.sub(r'\n|\t|\r', ' ', sentence)

def cleaning(sentence):
    """ A function to process a sentence in following order:
    1. lowercase all character
    2. normalize all vietnamese words
    3. remove all numbers
    4. remove punctuations
    5. expanding contractions
    6. replace special words and symbols
    7. remove duplicated ending letters in words
    8. remove tags
    ----------
    sentence : str
    input string

    Returns
    ----------
    str
    processed string
    """
    sentence = sentence.lower()
    sentence = text_normalize(sentence)
    sentence = remove_all_numbers(sentence)
    sentence = remove_punctuation(sentence) # delete punctuation
    sentence = expand_contractions(sentence, contraction_mapping = VIET_TAT) # fix shorthand
    sentence = replace_sent(sentence, replace_list)
    sentence = remove_duplicate_ending_letters(sentence)
    sentence =  remove_all_tag(sentence)
    return sentence